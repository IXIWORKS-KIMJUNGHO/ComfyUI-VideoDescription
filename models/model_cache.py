"""
Model Cache for Qwen3-VL
Singleton pattern to avoid loading models multiple times
"""

import torch
import os
import warnings
from pathlib import Path
from transformers import Qwen3VLForConditionalGeneration, AutoProcessor
from typing import Tuple, Optional
import logging

# Suppress known deprecation warnings from transformers library
warnings.filterwarnings('ignore', message='.*torchvision.*deprecated.*')
warnings.filterwarnings('ignore', category=FutureWarning, module='transformers')

logger = logging.getLogger(__name__)


class ModelCache:
    """
    Singleton cache for Qwen3-VL model and processor
    Ensures models are loaded only once per session
    """

    _instance = None
    _qwen3vl_model: Optional[Qwen3VLForConditionalGeneration] = None
    _qwen3vl_processor: Optional[AutoProcessor] = None
    _model_name = "Qwen/Qwen3-VL-8B-Instruct"

    @classmethod
    def _get_model_path(cls) -> Path:
        """
        Get the local model path in ComfyUI models directory

        Returns:
            Path to models/video_description/Qwen3-VL-8B-Instruct/
        """
        # Find ComfyUI root directory (go up from custom_nodes)
        current_file = Path(__file__).resolve()
        custom_nodes_dir = current_file.parent.parent.parent
        comfyui_root = custom_nodes_dir.parent

        # ComfyUI models directory
        model_path = comfyui_root / "models" / "video_description" / "Qwen3-VL-8B-Instruct"

        return model_path

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(ModelCache, cls).__new__(cls)
        return cls._instance

    @classmethod
    def get_qwen3vl(cls, use_4bit: bool = False) -> Tuple[Qwen3VLForConditionalGeneration, AutoProcessor]:
        """
        Get or load Qwen3-VL model and processor

        Args:
            use_4bit: Whether to use 4-bit quantization for memory efficiency

        Returns:
            Tuple of (model, processor)
        """
        if cls._qwen3vl_model is None or cls._qwen3vl_processor is None:
            # Get local model path
            model_path = cls._get_model_path()

            # Check if model exists locally in Hugging Face cache structure
            # HF downloads to: model_path/models--Qwen--Qwen3-VL-8B-Instruct/snapshots/[hash]/
            local_model_exists = False
            if model_path.exists():
                # Look for HF cache structure
                hf_cache_dirs = list(model_path.glob("models--Qwen--Qwen3-VL-8B-Instruct/snapshots/*"))
                if hf_cache_dirs:
                    # Use the HF cache directory
                    snapshot_dir = hf_cache_dirs[0]
                    if (snapshot_dir / "config.json").exists():
                        local_model_exists = True
                        model_source = str(snapshot_dir)
                        logger.info(f"Found model in HF cache: {snapshot_dir}")

            if not local_model_exists:
                logger.info(f"Local model not found at: {model_path}")
                logger.info(f"Will download from Hugging Face: {cls._model_name}")
                logger.info(f"Saving to: {model_path}")
                model_source = cls._model_name
                # Create directory if it doesn't exist
                model_path.mkdir(parents=True, exist_ok=True)

            logger.info(f"4-bit quantization: {use_4bit}")

            # Determine device
            if torch.cuda.is_available():
                device = "cuda"
                device_map = "auto"
            elif torch.backends.mps.is_available():
                device = "mps"
                device_map = None  # MPS doesn't support device_map
            else:
                device = "cpu"
                device_map = None

            logger.info(f"Using device: {device}")

            try:
                if use_4bit:
                    try:
                        from transformers import BitsAndBytesConfig

                        quantization_config = BitsAndBytesConfig(
                            load_in_4bit=True,
                            bnb_4bit_compute_dtype=torch.float16
                        )

                        logger.info("Loading model with 4-bit quantization...")
                        load_kwargs = {
                            "quantization_config": quantization_config,
                        }
                        if device_map:
                            load_kwargs["device_map"] = device_map
                        if not local_model_exists:
                            load_kwargs["cache_dir"] = str(model_path)

                        cls._qwen3vl_model = Qwen3VLForConditionalGeneration.from_pretrained(
                            model_source,
                            **load_kwargs
                        )
                    except ImportError as e:
                        logger.warning(f"bitsandbytes not available: {e}")
                        logger.warning("Falling back to FP16 loading. Install bitsandbytes for 4-bit quantization:")
                        logger.warning("  pip install bitsandbytes")

                        # Fall back to FP16
                        logger.info("Loading model in FP16...")
                        load_kwargs = {"dtype": torch.float16}
                        if device_map:
                            load_kwargs["device_map"] = device_map
                        if not local_model_exists:
                            load_kwargs["cache_dir"] = str(model_path)

                        cls._qwen3vl_model = Qwen3VLForConditionalGeneration.from_pretrained(
                            model_source,
                            **load_kwargs
                        )
                        if not device_map:
                            cls._qwen3vl_model = cls._qwen3vl_model.to(device)
                else:
                    logger.info("Loading model in FP16...")
                    load_kwargs = {"dtype": torch.float16}
                    if device_map:
                        load_kwargs["device_map"] = device_map
                    if not local_model_exists:
                        load_kwargs["cache_dir"] = str(model_path)

                    cls._qwen3vl_model = Qwen3VLForConditionalGeneration.from_pretrained(
                        model_source,
                        **load_kwargs
                    )
                    if not device_map:
                        cls._qwen3vl_model = cls._qwen3vl_model.to(device)

                processor_kwargs = {}
                if not local_model_exists:
                    processor_kwargs["cache_dir"] = str(model_path)

                cls._qwen3vl_processor = AutoProcessor.from_pretrained(
                    model_source,
                    **processor_kwargs
                )

                logger.info("âœ“ Model loaded successfully")
                logger.info(f"Model device: {cls._qwen3vl_model.device}")
                logger.info(f"Model location: {model_path}")

            except Exception as e:
                logger.error(f"Failed to load model: {e}")
                raise

        return cls._qwen3vl_model, cls._qwen3vl_processor

    @classmethod
    def clear_cache(cls):
        """Clear cached models to free memory"""
        logger.info("Clearing model cache")
        cls._qwen3vl_model = None
        cls._qwen3vl_processor = None

        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    @classmethod
    def is_loaded(cls) -> bool:
        """Check if model is already loaded"""
        return cls._qwen3vl_model is not None and cls._qwen3vl_processor is not None
